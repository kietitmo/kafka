{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'org.apache.spark:spark-sql-kafka-0-10_2.13:3.2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.16:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example-1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c32369e630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "scala_version = '2.13'\n",
    "spark_version = '3.4.3'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.2.0'\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"kafka-example-1\")\\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages))\\\n",
    "    .config(\"spark.jars\", \"./jars/postgresql-42.6.2.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Kafka\n",
    "TOPIC = os.getenv('TOPIC')\n",
    "BOOTSTRAP_SERVERS = os.getenv('BOOTSTRAP_SERVERS')\n",
    "\n",
    "# Postgres\n",
    "HOST = os.getenv('HOST')\n",
    "PORT =  os.getenv('PORT')\n",
    "TABLE = os.getenv('TABLE')\n",
    "URL = os.getenv('URL')\n",
    "DRIVER = os.getenv('DRIVER')\n",
    "USER = os.getenv('USER')\n",
    "PWD = os.getenv('PWD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS)\\\n",
    "  .option(\"subscribe\", TOPIC)\\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "json_schema = StructType() \\\n",
    "    .add(\"gender\", StringType()) \\\n",
    "    .add(\"name\", StructType()\n",
    "         .add(\"title\", StringType())\n",
    "         .add(\"first\", StringType())\n",
    "         .add(\"last\", StringType())) \\\n",
    "    .add(\"location\", StructType()\n",
    "         .add(\"street\", StructType()\n",
    "              .add(\"number\", StringType())\n",
    "              .add(\"name\", StringType()))\n",
    "         .add(\"city\", StringType())\n",
    "         .add(\"state\", StringType())\n",
    "         .add(\"country\", StringType())\n",
    "         .add(\"postcode\", StringType())\n",
    "         .add(\"coordinates\", StructType()\n",
    "              .add(\"latitude\", StringType())\n",
    "              .add(\"longitude\", StringType()))\n",
    "         .add(\"timezone\", StructType()\n",
    "              .add(\"offset\", StringType())\n",
    "              .add(\"description\", StringType()))) \\\n",
    "    .add(\"email\", StringType()) \\\n",
    "    .add(\"login\", StructType()\n",
    "         .add(\"uuid\", StringType())\n",
    "         .add(\"username\", StringType())\n",
    "         .add(\"password\", StringType())\n",
    "         .add(\"salt\", StringType())\n",
    "         .add(\"md5\", StringType())\n",
    "         .add(\"sha1\", StringType())\n",
    "         .add(\"sha256\", StringType())) \\\n",
    "    .add(\"dob\", StructType()\n",
    "         .add(\"date\", StringType())\n",
    "         .add(\"age\", StringType())) \\\n",
    "    .add(\"registered\", StructType()\n",
    "         .add(\"date\", StringType())\n",
    "         .add(\"age\", StringType())) \\\n",
    "    .add(\"phone\", StringType()) \\\n",
    "    .add(\"cell\", StringType()) \\\n",
    "    .add(\"id\", StructType()\n",
    "         .add(\"name\", StringType())\n",
    "         .add(\"value\", StringType())) \\\n",
    "    .add(\"picture\", StructType()\n",
    "         .add(\"large\", StringType())\n",
    "         .add(\"medium\", StringType())\n",
    "         .add(\"thumbnail\", StringType())) \\\n",
    "    .add(\"nat\", StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), json_schema).alias(\"parsed_value\")).select(\"parsed_value.*\")\n",
    "\n",
    "# Transform data\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"gender\"),\n",
    "    col(\"name.first\").alias(\"first_name\"),\n",
    "    col(\"name.last\").alias(\"last_name\"),\n",
    "    col(\"location.city\").alias(\"city\"),\n",
    "    col(\"location.country\").alias(\"country\"),\n",
    "    col(\"email\"),\n",
    "    col(\"login.username\").alias(\"username\"),\n",
    "    col(\"registered.date\").cast(TimestampType()).alias(\"registered_date\"),\n",
    "    col(\"dob.date\").cast(TimestampType()).alias(\"dob\"),\n",
    "    col(\"phone\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = flattened_df \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"json\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"path\", \"output\") \\\n",
    "#     .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "#     .option(\"failOnDataLoss\", \"false\") \\\n",
    "#     .trigger(processingTime=\"1 minute\") \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_postgresql(df,epoch_id):\n",
    "    df.write \\\n",
    "    .format('jdbc') \\\n",
    "    .options(url=URL,\n",
    "            driver=DRIVER,\n",
    "            dbtable=TABLE,\n",
    "            user=USER,\n",
    "            password=PWD,\n",
    "            ) \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = flattened_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(write_to_postgresql)\\\n",
    "    .outputMode('update') \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phần bên dưới dùng python bình thường chạy consumer ok nhưng pyspark phía trên ko đc => lỗi không phải do kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "        'user-topic',\n",
    "        bootstrap_servers='localhost:9092',\n",
    "        auto_offset_reset='earliest'\n",
    "    )\n",
    "for message in consumer:\n",
    "    print(json.loads(message.value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
